{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2510329,"sourceType":"datasetVersion","datasetId":1520310},{"sourceType":"kernelVersion","sourceId":9102236}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nimport gensim\nfrom fuzzywuzzy import fuzz\nimport itertools\nimport matplotlib.pyplot as plt\nimport time\n\n# NLTK Data\nnltk.download('stopwords')\n\n# Settings\nTEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\nSEQUENCE_LENGTH = 300\nEPOCHS = 8\nBATCH_SIZE = 1024\nTRAIN_SIZE = 0.8\nW2V_SIZE = 300\nW2V_WINDOW = 7\nW2V_EPOCH = 32\nW2V_MIN_COUNT = 10\nSENTIMENT_THRESHOLDS = (0.4, 0.7)\nKERAS_MODEL = \"model.h5\"\nWORD2VEC_MODEL = \"model.w2v\"\nTOKENIZER_MODEL = \"tokenizer.pkl\"\nENCODER_MODEL = \"encoder.pkl\"\n\n# Load Dataset\ndf = pd.read_csv('/kaggle/input/twitter-entity-sentiment-analysis/twitter_training.csv', names=[\"id\", \"category\", \"label\", \"text\"])\n\n# Preprocess Text\nstop_words = stopwords.words(\"english\")\nstemmer = SnowballStemmer(\"english\")\n\ndef preprocess(text, stem=False):\n    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n    tokens = []\n    for token in text.split():\n        if token not in stop_words:\n            if stem:\n                tokens.append(stemmer.stem(token))\n            else:\n                tokens.append(token)\n    return \" \".join(tokens)\n\ndf.text = df.text.apply(lambda x: preprocess(x))\n\ndf_train, df_test = train_test_split(df, test_size=1-TRAIN_SIZE, random_state=42)\n\n# Word2Vec Model\ndocuments = [_text.split() for _text in df_train.text]\n\nw2v_model = gensim.models.Word2Vec(size=W2V_SIZE,\n                                   window=W2V_WINDOW,\n                                   min_count=W2V_MIN_COUNT,\n                                   workers=8)\nw2v_model.build_vocab(documents)\nw2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)\n\nprint(\"Vocab size\", len(w2v_model.wv.vocab.keys()))\nprint(w2v_model.most_similar(\"love\"))\n\n# Tokenization and Label Encoding\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df_train.text)\nvocab_size = len(tokenizer.word_index) + 1\n\nx_train = pad_sequences(tokenizer.texts_to_sequences(df_train.text), maxlen=SEQUENCE_LENGTH)\nx_test = pad_sequences(tokenizer.texts_to_sequences(df_test.text), maxlen=SEQUENCE_LENGTH)\n\nlabels = df_train.label.unique().tolist()\nencoder = LabelEncoder()\nencoder.fit(df_train.label.tolist())\n\ny_train = encoder.transform(df_train.label.tolist())\ny_test = encoder.transform(df_test.label.tolist())\ny_train = y_train.reshape(-1, 1)\ny_test = y_test.reshape(-1, 1)\n\n# Embedding Layer\nembedding_matrix = np.zeros((vocab_size, W2V_SIZE))\nfor word, i in tokenizer.word_index.items():\n    if word in w2v_model.wv:\n        embedding_matrix[i] = w2v_model.wv[word]\n\nembedding_layer = Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)\n\n# Build and Compile the Neural Network Model\nmodel = Sequential()\nmodel.add(embedding_layer)\nmodel.add(Dropout(0.5))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\nmodel.summary()\n\n# Training the Model\ncallbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n             EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]\n\nhistory = model.fit(x_train, y_train,\n                    batch_size=BATCH_SIZE,\n                    epochs=EPOCHS,\n                    validation_split=0.1,\n                    verbose=1,\n                    callbacks=callbacks)\n\n# Model Evaluation\nscore = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\nprint(\"ACCURACY:\", score[1])\nprint(\"LOSS:\", score[0])\n\n# Plot training history\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'b', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n# Predict and Decode Sentiment\ndef decode_sentiment(score, include_neutral=True):\n    if include_neutral:\n        if score <= SENTIMENT_THRESHOLDS[0]:\n            return \"NEGATIVE\"\n        elif score >= SENTIMENT_THRESHOLDS[1]:\n            return \"POSITIVE\"\n        else:\n            return \"NEUTRAL\"\n    else:\n        return \"NEGATIVE\" if score < 0.5 else \"POSITIVE\"\n\ndef predict(text, include_neutral=True):\n    start_at = time.time()\n    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n    score = model.predict([x_test])[0]\n    label = decode_sentiment(score, include_neutral=include_neutral)\n    return {\"label\": label, \"score\": float(score), \"elapsed_time\": time.time() - start_at}\n\nprint(predict(\"I love the music\"))\nprint(predict(\"I hate the rain\"))\nprint(predict(\"I don't know what I'm doing\"))\n\n# Confusion Matrix and Classification Report\ny_pred_1d = [decode_sentiment(score, include_neutral=False) for score in model.predict(x_test, verbose=1, batch_size=8000)]\ny_test_1d = list(df_test.label)\n\ndef plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=30)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90, fontsize=22)\n    plt.yticks(tick_marks, classes, fontsize=22)\n    fmt = '.2f'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.ylabel('True label', fontsize=25)\n    plt.xlabel('Predicted label', fontsize=25)\n\ncnf_matrix = confusion_matrix(y_test_1d, y_pred_1d)\nplt.figure(figsize=(12, 12))\nplot_confusion_matrix(cnf_matrix, classes=df_train.label.unique(), title=\"Confusion matrix\")\nplt.show()\n\nprint(classification_report(y_test_1d, y_pred_1d))\nprint(\"Accuracy Score:\", accuracy_score(y_test_1d, y_pred_1d))\n\n# Fuzzy Logic Integration using fuzzywuzzy\ndef fuzzy_sentiment_analysis(score):\n    low = fuzz.ratio(score, 'NEGATIVE')\n    medium = fuzz.ratio(score, 'NEUTRAL')\n    high = fuzz.ratio(score, 'POSITIVE')\n    fuzzy_score = np.zeros_like(score)\n    fuzzy_score[score < SENTIMENT_THRESHOLDS[0]] = low\n    fuzzy_score[(score >= SENTIMENT_THRESHOLDS[0]) & (score <= SENTIMENT_THRESHOLDS[1])] = medium\n    fuzzy_score[score > SENTIMENT_THRESHOLDS[1]] = high\n    return fuzzy_score\n\n# Example fuzzy analysis\ntext = \"I love the music\"\nprediction = predict(text)\nfuzzy_score = fuzzy_sentiment_analysis(prediction[\"score\"])\nprint(f\"Fuzzy analysis for '{text}': {fuzzy_score}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-28T04:53:20.398033Z","iopub.execute_input":"2024-07-28T04:53:20.398510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nimport gensim\nfrom fuzzywuzzy import fuzz\nimport itertools\nimport matplotlib.pyplot as plt\nimport time\n\n# NLTK Data\nnltk.download('stopwords')\n\n# Settings\nTEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\nSEQUENCE_LENGTH = 300\nEPOCHS = 8\nBATCH_SIZE = 1024\nTRAIN_SIZE = 0.8\nW2V_SIZE = 300\nW2V_WINDOW = 7\nW2V_EPOCH = 32\nW2V_MIN_COUNT = 10\nSENTIMENT_THRESHOLDS = (0.4, 0.7)\nKERAS_MODEL = \"model.h5\"\nWORD2VEC_MODEL = \"model.w2v\"\nTOKENIZER_MODEL = \"tokenizer.pkl\"\nENCODER_MODEL = \"encoder.pkl\"\n\n# Load Dataset\ndf = pd.read_csv('/kaggle/input/twitter-entity-sentiment-analysis/twitter_training.csv', names=[\"id\", \"category\", \"label\", \"text\"])\n\n# Preprocess Text\nstop_words = stopwords.words(\"english\")\nstemmer = SnowballStemmer(\"english\")\n\ndef preprocess(text, stem=False):\n    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n    tokens = []\n    for token in text.split():\n        if token not in stop_words:\n            if stem:\n                tokens.append(stemmer.stem(token))\n            else:\n                tokens.append(token)\n    return \" \".join(tokens)\n\ndf.text = df.text.apply(lambda x: preprocess(x))\n\ndf_train, df_test = train_test_split(df, test_size=1-TRAIN_SIZE, random_state=42)\n\n# Word2Vec Model\ndocuments = [_text.split() for _text in df_train.text]\n\nw2v_model = gensim.models.Word2Vec(size=W2V_SIZE,\n                                   window=W2V_WINDOW,\n                                   min_count=W2V_MIN_COUNT,\n                                   workers=8)\nw2v_model.build_vocab(documents)\nw2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)\n\nprint(\"Vocab size\", len(w2v_model.wv.vocab.keys()))\nprint(w2v_model.most_similar(\"love\"))\n\n# Tokenization and Label Encoding\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df_train.text)\nvocab_size = len(tokenizer.word_index) + 1\n\nx_train = pad_sequences(tokenizer.texts_to_sequences(df_train.text), maxlen=SEQUENCE_LENGTH)\nx_test = pad_sequences(tokenizer.texts_to_sequences(df_test.text), maxlen=SEQUENCE_LENGTH)\n\nlabels = df_train.label.unique().tolist()\nencoder = LabelEncoder()\nencoder.fit(df_train.label.tolist())\n\ny_train = encoder.transform(df_train.label.tolist())\ny_test = encoder.transform(df_test.label.tolist())\ny_train = y_train.reshape(-1, 1)\ny_test = y_test.reshape(-1, 1)\n\n# Embedding Layer\nembedding_matrix = np.zeros((vocab_size, W2V_SIZE))\nfor word, i in tokenizer.word_index.items():\n    if word in w2v_model.wv:\n        embedding_matrix[i] = w2v_model.wv[word]\n\nembedding_layer = Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)\n\n# Build and Compile the Neural Network Model\nmodel = Sequential()\nmodel.add(embedding_layer)\nmodel.add(Dropout(0.5))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\nmodel.summary()\n\n# Training the Model\ncallbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n             EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]\n\nhistory = model.fit(x_train, y_train,\n                    batch_size=BATCH_SIZE,\n                    epochs=EPOCHS,\n                    validation_split=0.1,\n                    verbose=1,\n                    callbacks=callbacks)\n\n# Model Evaluation\nscore = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\nprint(\"ACCURACY:\", score[1])\nprint(\"LOSS:\", score[0])\n\n# Plot training history\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'b', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n# Predict and Decode Sentiment\ndef decode_sentiment(score, include_neutral=True):\n    if include_neutral:\n        if score <= SENTIMENT_THRESHOLDS[0]:\n            return \"NEGATIVE\"\n        elif score >= SENTIMENT_THRESHOLDS[1]:\n            return \"POSITIVE\"\n        else:\n            return \"NEUTRAL\"\n    else:\n        return \"NEGATIVE\" if score < 0.5 else \"POSITIVE\"\n\ndef predict(text, include_neutral=True):\n    start_at = time.time()\n    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n    score = model.predict([x_test])[0]\n    label = decode_sentiment(score, include_neutral=include_neutral)\n    return {\"label\": label, \"score\": float(score), \"elapsed_time\": time.time() - start_at}\n\nprint(predict(\"I love the music\"))\nprint(predict(\"I hate the rain\"))\nprint(predict(\"I don't know what I'm doing\"))\n\n# Confusion Matrix and Classification Report\ny_pred_1d = [decode_sentiment(score, include_neutral=False) for score in model.predict(x_test, verbose=1, batch_size=8000)]\ny_test_1d = list(df_test.label)\n\ndef plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=30)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90, fontsize=22)\n    plt.yticks(tick_marks, classes, fontsize=22)\n    fmt = '.2f'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.ylabel('True label', fontsize=25)\n    plt.xlabel('Predicted label', fontsize=25)\n\ncnf_matrix = confusion_matrix(y_test_1d, y_pred_1d)\nplt.figure(figsize=(12, 12))\nplot_confusion_matrix(cnf_matrix, classes=df_train.label.unique(), title=\"Confusion matrix\")\nplt.show()\n\nprint(classification_report(y_test_1d, y_pred_1d))\nprint(\"Accuracy Score:\", accuracy_score(y_test_1d, y_pred_1d))\n\n# Fuzzy Logic Integration using fuzzywuzzy\ndef fuzzy_sentiment_analysis(score):\n    low = fuzz.ratio(score, 'NEGATIVE')\n    medium = fuzz.ratio(score, 'NEUTRAL')\n    high = fuzz.ratio(score, 'POSITIVE')\n    fuzzy_score = np.zeros_like(score)\n    fuzzy_score[score < SENTIMENT_THRESHOLDS[0]] = low\n    fuzzy_score[(score >= SENTIMENT_THRESHOLDS[0]) & (score <= SENTIMENT_THRESHOLDS[1])] = medium\n    fuzzy_score[score > SENTIMENT_THRESHOLDS[1]] = high\n    return fuzzy_score\n\n# Example fuzzy analysis\ntext = \"I love the music\"\nprediction = predict(text)\nfuzzy_score = fuzzy_sentiment_analysis(prediction[\"score\"])\nprint(f\"Fuzzy analysis for '{text}': {fuzzy_score}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}